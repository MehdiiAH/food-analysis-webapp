{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8161a8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c5b536",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_recipe = pd.read_csv(\"data/raw/RAW_recipes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391447a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_recipe.head()\n",
    "data_text=data_recipe[['name','description']]\n",
    "data_text.head()\n",
    "\n",
    "doc=nlp(data_text['description'][0])\n",
    "\n",
    "print(doc)\n",
    "\n",
    "print(\"\\nTokens\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.head.text)\n",
    "\n",
    "print(\"\\n Sentences\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n",
    "\n",
    "print(\"\\n Chunks\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, \"→\", chunk.root.text)\n",
    "\n",
    "print(\"\\n Dependencies\")\n",
    "#spacy.displacy.render(doc, style=\"dep\", jupyter=True)\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<10} ←{token.dep_:<10}– {token.head.text}\")\n",
    "\n",
    "print(\"\\n Pipeline\")\n",
    "for name, component in nlp.pipeline:\n",
    "    print(name, type(component))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5e1980",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Charger modèle sans NER\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "# DataFrame avec texte\n",
    "# data_recipe = pd.read_csv(\"recipes.csv\")\n",
    "data_text = data_recipe[['name', 'description']].dropna()\n",
    "\n",
    "# Fonction optimisée (sans créer doc ici)\n",
    "def extract_features(doc):\n",
    "    lemmas = [t.lemma_.lower() for t in doc if t.is_alpha and t.text.lower() not in stopwords]\n",
    "    noun_chunks = [c.text.lower() for c in doc.noun_chunks]\n",
    "    pos_counts = doc.count_by(spacy.attrs.POS)\n",
    "    pos_dict = {doc.vocab[i].text: count for i, count in pos_counts.items()}\n",
    "    return {\"lemmas\": lemmas, \"noun_chunks\": noun_chunks, \"pos_counts\": pos_dict}\n",
    "\n",
    "# Traitement par lots (streaming + multi-process)\n",
    "texts = data_text[\"description\"].tolist()\n",
    "features = []\n",
    "\n",
    "for doc in nlp.pipe(texts, batch_size=50, n_process=4):  # n_process = nb de CPU\n",
    "    features.append(extract_features(doc))\n",
    "\n",
    "# Assemblage\n",
    "data_text[\"features\"] = features\n",
    "data_text[\"tokens\"] = [f[\"lemmas\"] + f[\"noun_chunks\"] for f in features]\n",
    "\n",
    "# Export\n",
    "output_df = data_text[['name', 'description', 'tokens']]\n",
    "output_df.to_csv(\"data_recipe_features.csv\", index=False)\n",
    "print(\"✅ Fichier 'data_recipe_features.csv' sauvegardé avec succès !\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "food-analysis-webapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
