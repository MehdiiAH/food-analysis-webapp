{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "8161a8ba",
         "metadata": {},
         "outputs": [],
         "source": [
            "import pandas as pd\n",
            "import spacy\n",
            "\n",
            "\n",
            "nlp = spacy.load(\"en_core_web_sm\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "e4c5b536",
         "metadata": {},
         "outputs": [],
         "source": [
            "data_recipe = pd.read_csv(\"data/raw/RAW_recipes.csv\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "391447a2",
         "metadata": {},
         "outputs": [],
         "source": [
            "data_recipe.head()\n",
            "data_text=data_recipe[['name','description']]\n",
            "data_text.head()\n",
            "\n",
            "doc=nlp(data_text['description'][0])\n",
            "\n",
            "print(doc)\n",
            "\n",
            "print(\"\\nTokens\")\n",
            "for token in doc:\n",
            "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.head.text)\n",
            "\n",
            "print(\"\\n Sentences\")\n",
            "for sent in doc.sents:\n",
            "    print(sent.text)\n",
            "\n",
            "print(\"\\n Chunks\")\n",
            "for chunk in doc.noun_chunks:\n",
            "    print(chunk.text, \"→\", chunk.root.text)\n",
            "\n",
            "print(\"\\n Dependencies\")\n",
            "#spacy.displacy.render(doc, style=\"dep\", jupyter=True)\n",
            "for token in doc:\n",
            "    print(f\"{token.text:<10} ←{token.dep_:<10}– {token.head.text}\")\n",
            "\n",
            "print(\"\\n Pipeline\")\n",
            "for name, component in nlp.pipeline:\n",
            "    print(name, type(component))\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "2e5e1980",
         "metadata": {},
         "outputs": [],
         "source": [
            "import spacy\n",
            "import pandas as pd\n",
            "\n",
            "# Charger modèle sans NER\n",
            "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\n",
            "stopwords = nlp.Defaults.stop_words\n",
            "\n",
            "# DataFrame avec texte\n",
            "data_text = data_recipe[['name', 'description']].dropna()\n",
            "\n",
            "def extract_features(doc):\n",
            "    lemmas = [t.lemma_.lower() for t in doc if t.is_alpha and t.text.lower() not in stopwords]\n",
            "    noun_chunks = [c.text.lower() for c in doc.noun_chunks]\n",
            "    return lemmas + noun_chunks\n",
            "\n",
            "# Traitement par lots\n",
            "tokens_list = []\n",
            "for doc in nlp.pipe(data_text[\"description\"].tolist(), batch_size=50, n_process=4):\n",
            "    tokens_list.append(extract_features(doc))\n",
            "\n",
            "# Ajouter la colonne tokens au DataFrame original\n",
            "data_recipe.loc[data_text.index, \"tokens\"] = pd.Series(tokens_list, index=data_text.index)\n",
            "\n",
            "# Sauvegarder sous un nouveau nom\n",
            "data_recipe.to_csv(\"data_recipe_with_tokens.csv\", index=False)\n",
            "print(\"✅ Fichier 'data_recipe_with_tokens.csv' sauvegardé avec succès !\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "3d88f7aa",
         "metadata": {},
         "outputs": [],
         "source": [
            "data_recipe = pd.read_csv(\"data/raw/RAW_recipes.csv\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "11958d42",
         "metadata": {},
         "outputs": [],
         "source": [
            "data_recipe.head()\n",
            "data_text = data_recipe[[\"name\", \"description\"]]\n",
            "data_text.head()\n",
            "\n",
            "doc = nlp(data_text[\"description\"][0])\n",
            "\n",
            "print(doc)\n",
            "\n",
            "print(\"\\nTokens\")\n",
            "for token in doc:\n",
            "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.head.text)\n",
            "\n",
            "print(\"\\n Sentences\")\n",
            "for sent in doc.sents:\n",
            "    print(sent.text)\n",
            "\n",
            "print(\"\\n Chunks\")\n",
            "for chunk in doc.noun_chunks:\n",
            "    print(chunk.text, \"→\", chunk.root.text)\n",
            "\n",
            "print(\"\\n Dependencies\")\n",
            "# spacy.displacy.render(doc, style=\"dep\", jupyter=True)\n",
            "for token in doc:\n",
            "    print(f\"{token.text:<10} ←{token.dep_:<10}– {token.head.text}\")\n",
            "\n",
            "print(\"\\n Pipeline\")\n",
            "for name, component in nlp.pipeline:\n",
            "    print(name, type(component))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "58b015c0",
         "metadata": {},
         "outputs": [],
         "source": [
            "# Small example\n",
            "data_text[\"NLP_Desc\"] = data_text[\"description\"]\n",
            "data_text.head()\n",
            "for i in range(5):\n",
            "    data_text[\"NLP_Desc\"][i] = nlp(data_text[\"description\"][i])\n",
            "\n",
            "data_text.head(5)\n",
            "data_tfidf = data_text.iloc[:5]\n",
            "\n",
            "print(data_tfidf)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "e5378a14",
         "metadata": {},
         "outputs": [],
         "source": [
            "data_tfidf[\"NLP_Selection\"] = data_tfidf[\"NLP_Desc\"]\n",
            "for i in range(5):\n",
            "    lemmes = [\n",
            "        token.lemma_\n",
            "        for token in data_tfidf[\"NLP_Desc\"][i]\n",
            "        if token.pos_ in [\"NOUN\", \"ADJ\"]\n",
            "    ]\n",
            "    # print(lemmes)\n",
            "    data_tfidf[\"NLP_Selection\"][i] = lemmes\n",
            "# print(data_tfidf)\n",
            "\n",
            "data_tfidf.head(5)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "4cbac622",
         "metadata": {},
         "outputs": [],
         "source": [
            "import numpy as np\n",
            "\n",
            "texts = [t for t in data_tfidf[\"NLP_Selection\"].tolist() if len(t) > 0]\n",
            "n_samples = len(texts)\n",
            "\n",
            "# If the vocabulary is not known, we need to build it\n",
            "words = set()\n",
            "for t in texts:\n",
            "    words = words.union(set(t))\n",
            "n_features = len(words)\n",
            "vocabulary = dict(zip(words, range(n_features)))\n",
            "\n",
            "# Creating the matrix counts\n",
            "counts = np.zeros((n_samples, n_features))\n",
            "\n",
            "# Filling the matrix by iterating over the documents and counting the words\n",
            "for k, t in enumerate(texts):\n",
            "    for w in t:\n",
            "        counts[k][vocabulary[w]] += 1.0"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "294c6180",
         "metadata": {},
         "outputs": [],
         "source": [
            "searched_text = [\"spicy\", \"sausage\"]\n",
            "searched_vector = np.zeros(len(vocabulary), dtype=int)\n",
            "\n",
            "# Mettre +1 dans la colonne correspondant à chaque mot présent\n",
            "for word in searched_text:\n",
            "    if word in vocabulary:  # Vérifie que le mot existe dans le vocabulaire\n",
            "        searched_vector[vocabulary[word]] += 1\n",
            "\n",
            "print(searched_vector)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "8d36f81c",
         "metadata": {},
         "outputs": [],
         "source": [
            "print(texts)\n",
            "print(vocabulary)\n",
            "print(counts)\n",
            "print(counts.shape)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "4338ba06",
         "metadata": {},
         "outputs": [],
         "source": [
            "from sklearn.preprocessing import normalize\n",
            "\n",
            "d = counts.shape[0]\n",
            "# in_doc consiste à compter le nombre de valeurs positives par colonne, soit le nombre de documents dans lequel le mot apparait au moins une fois\n",
            "in_doc = np.count_nonzero(counts > 0, axis=0)\n",
            "# idf est une division terme à terme entre d et in_doc à laquelle on applique le log.\n",
            "idf = np.log(d / in_doc)\n",
            "\n",
            "# TF\n",
            "\n",
            "# sum_vec est la somme des termes d'une ligne, soit le nombre de mots pour chaque document.\n",
            "sum_vec = counts.sum(axis=1, keepdims=True) + 10**-5\n",
            "\n",
            "# tf est la division pour chaque coefficient, de l'occurrence d'un mot dans le document sur l'ensemble des mots d'un document, soit bow/sum_vec\n",
            "tf = np.divide(counts, sum_vec)\n",
            "\n",
            "# tf_idf est le produit de tf et idf, que l'on normalise en prévision de la suite.\n",
            "tf_idf = tf * idf\n",
            "normalize(tf_idf)\n",
            "\n",
            "print(tf_idf)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "84f2e7c3",
         "metadata": {},
         "outputs": [],
         "source": [
            "normalized_searched_vector = (\n",
            "    searched_vector / searched_vector.sum(axis=0, keepdims=True) * idf\n",
            ")\n",
            "print(normalized_searched_vector.shape)\n",
            "print(normalized_searched_vector)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "748ef858",
         "metadata": {},
         "outputs": [],
         "source": [
            "def euclidean(u, v):\n",
            "    return np.linalg.norm(u - v)\n",
            "\n",
            "def length_norm(u):\n",
            "    return u / np.sqrt(u.dot(u))\n",
            "\n",
            "def cosine(u, v):\n",
            "    return 1.0 - length_norm(u).dot(length_norm(v))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "fb12b2c6",
         "metadata": {},
         "outputs": [],
         "source": [
            "from sklearn.neighbors import NearestNeighbors\n",
            "\n",
            "X_input = normalized_searched_vector.reshape(1, -1)\n",
            "print(X_input)\n",
            "\n",
            "nn = NearestNeighbors(n_neighbors=5, metric=cosine)\n",
            "nn.fit(tf_idf)\n",
            "distances, indices = nn.kneighbors(X_input)\n",
            "\n",
            "print(indices)\n",
            "print(distances)"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "food-analysis-webapp",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.13"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 5
}
